---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am Yu Liu (åˆ˜æ¯“), a final-year M.S. candidate in the [School of Intelligent Science and Technology](http://hias.ucas.ac.cn/znkxyjs/index.htm), [Hangzhou Institute for Advanced Study (HIAS)](http://hias.ucas.ac.cn/), [University of Chinese Academy of Sciences (UCAS)](https://www.ucas.edu.cn/).

My research interests include long-lived, human-aware multimodal agents with a robust perceptionâ†’cognitionâ†’feedback loopâ€”via multimodal affective computing, cross-modal alignment, long-horizon memory & reasoning, multi-agent coordination, and safe embodied interaction.


<link href="https://fonts.googleapis.com/css2?family=Inter:wght@500;600&display=swap" rel="stylesheet">

<p class="phd-banner">
  Iâ€™m currently seeking PhD positions (Fall 2026 start) aligned with the focus above; Iâ€™m happy to connect regarding well-matched openings.
</p>

<style>
/* ä»…å½±å“è¿™æ¡å£°æ˜ï¼Œä¸æ”¹å˜å…¨ç«™æ ·å¼ */
.phd-banner{
  font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  font-weight: 600;           /* æ¯”æ­£æ–‡ç•¥é‡ï¼Œæ˜“æ‰«è¯» */
  letter-spacing: 0.1px;      /* è½»å¾®å­—è·ï¼Œæå‡å¯è¯»æ€§ */
  margin: 0.4rem 0 0.2rem;    /* ä¸å‰åæ®µè½ç•™ç‚¹å‘¼å¸æ„Ÿ */
}
</style>

I am very fortunate to be advised by [Prof. Taihao Li](https://people.ucas.ac.cn/~0070909) and [Dr. Leyuan Qu](https://people.ucas.edu.cn/~leyuanqu).

You can find my CV here: [Yu Liu's Curriculum Vitae](../assets/Yu_Liu_CV.pdf) (Updated: Dec 2, 2025).

<!-- [Email](mailto:liuyu233@mails.ucas.ac.cn) / [Github](https://github.com/YultheConkor) / [LinkedIn](https://www.linkedin.com/in/yu-liu-1b8004238/) -->

<section id="research-highlights">
  <h1>Research Highlights</h1>
  <p class="legend" aria-label="Legend">
  <em>Legend:</em>
  <span role="img" aria-label="Lead contribution">ğŸŒŸ</span> Lead contribution
  </p>
  <h2>Affective & Social AI</h2>
  <style>
  .legend { margin: 4px 0 14px; font-size: 0.92rem; color: #666; }
  .legend em { font-style: normal; font-weight: 600; margin-right: .5rem; color: #555; }
</style>
  <p><em>Research Highlights â€” Core: Multimodal Empathetic Dialogue Agent</em><br>
  <small>Role: Lead for multimodal fusion across textâ€“audioâ€“visual streams.</small><br>
  <small>Agent pipeline: Perception â†’ Dialogue Understanding â†’ Expression</small></p>

  <ul>
    <li>
      <strong>GRACE for Multimodal Facial Emotion Recognition</strong>
      [IEEE Transactions on Affective Computing <em>under review</em>, 
      <a href="https://arxiv.org/abs/2507.11892" target="_blank" rel="noopener">arXiv</a>] ğŸŒŸ:
      <br>
      Introduces <em>GRACE</em> for dynamic facial emotion recognition by aligning refined linguistic cues with salient facial dynamics; achieves SOTA on DFEW, FERV39k, and MAFW.
      <em>Agent module: instant emotion perception for the dialogue agent.</em>
    </li>

    <li>
      <strong>Centering Emotion Hotspots</strong>
      [under review; <a href="https://arxiv.org/abs/2510.08606" target="_blank" rel="noopener">arXiv (Oct 2025)</a>] ğŸŒŸ:
      <br>
      Extends GRACE from frame-level signals to conversation-level tracking by centering multimodal hotspots and fusing them with dialogue context, improving stability and generalization across turns.
      <em>Agent module: dialogue-level emotion understanding.</em>
    </li>

    <li>
      <strong>Think-Before-Draw</strong>
      [Pattern Recognition <em>under review</em> Â· 
      <a href="https://arxiv.org/abs/2507.12761" target="_blank" rel="noopener">arXiv</a>]:
      <br>
      A two-stage framework for disentangled, controllable talking-head synthesis. Designed the multimodal fusion module integrating audioâ€“textâ€“visual cues to preserve identity and sharpen affect control.
      <em>Agent module: controllable affective expression for responses.</em>
    </li>
  </ul>

  <h2>Digital Therapeutics</h2>

  
  <p><strong>Zhejiang Vanguard Project: Digital Therapeutics for Depression Detection</strong><br>
  <em>Research Highlight: Unified visual biomarker extraction with LLM-augmented questionnaire semantics via multimodal fusion for depression screening under privacy/low-resource constraints; <strong>incubated HOPE</strong> for subject-level estimation.</em> <br>
  <small>Role: Led multimodal fusion (video/audio/text); built the visual-biomarker pipeline; drove cross-module integration aligned to subject-level decisions.</small>
  </p>

<ul>
  <li>
    <strong>HOPE: Hierarchical Fusion for Optimized and Personality-Aware Estimation of Depression</strong>
    [<a href="https://doi.org/10.1145/3746027.3762063" target="_blank" rel="noopener">ACM MM â€™25 Â· MPDD Challenge (Young Track) Â· DOI</a> Â·
     <a href="https://github.com/YultheConkor/HOPE" target="_blank" rel="noopener">GitHub</a>] ğŸŒŸ:
    <br>
    Subject-level depression detection under privacy constraints via hierarchical multimodal fusion (audioâ€“video with personalized textual cues) and cross-task/sample consistencyâ€”<strong>1st place</strong> in the MPDD Young Track.<br>
    <em>Role:</em> Developed the consistency-aware subject-level fusion strategy; led the writing; completed final code integration and open-source release preparation.
  </li>
</ul>

  <h2>Earlier Work: Aviation Analytics</h2>
  <ul>
    <li>
      <strong>Risk Propagation on Flight Networks</strong>
      [<a href="http://www.tseit.org.cn/CN/Y2020/V20/I1/198" target="_blank" rel="noopener">Journal of Transportation Systems Engineering (2020) Â· DOI</a> 2023 Frontrunner 5000 Top Article]ğŸŒŸ:<br>
      Introduced Spearman correlation + SIR dynamics into airline operational risk management, yielding deployable complex-network propagation analysis.
    </li>
  </ul>
</section>

<section id="education">
  <h1>Education</h1>
  <ul>
    <li>
      University of Chinese Academy of Sciences â€” <strong>M.S. in Artificial Intelligence</strong>, Hangzhou Institute for Advanced Study (GPA 3.76/4.00),
      <em>Sep 2023 â€“ Jun 2026 (expected)</em>.
    </li>
    <li>
      Civil Aviation University of China â€” <strong>B.S. in Transportation</strong> (GPA 3.36/4.00),
      <em>Sep 2015 â€“ Jun 2019</em>.
    </li>
  </ul>
</section>

<section id="work-experience">
  <h1>Work Experience</h1>

  <h2>China Southern Airlines â€” Data Analyst, Airlines Operations Center <span style="font-weight:normal;">(Sep 2019 â€“ May 2023)</span></h2>
  <ul>
    <li>
      Built Python/MySQL analytics and monitoring; automated Tableau/QuickBI reporting, reducing manual report time by <strong>&gt;70%</strong> and enabling shared data services across departments.
    </li>
    <li>
      Led a Django+SQL delay/fault analytics platform, cutting report generation time by <strong>~85%</strong> and supporting operational safety and decision-making.
    </li>
  </ul>

  <h2>Hangzhou Institute for Advanced Study, UCAS â€” Lab Coordinator &amp; Administrator <span style="font-weight:normal;">(Sep 2024 â€“ Present)</span></h2>
  <ul>
    <li>
      Managed onboarding, event organization, and <strong>GPU resource</strong> scheduling; oversaw equipment procurement and reimbursement workflows to keep projects moving.
    </li>
  </ul>
</section>

<section id="professional-service">
  <h1>Professional Service</h1>
  <ul>
    <li><strong>Journal Reviewer</strong>: <em>Pattern Recognition</em> (Elsevier), <em>2025â€“present</em>.</li>
  </ul>
</section>