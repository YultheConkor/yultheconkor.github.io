---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am Yu Liu (刘毓), a final-year M.S. candidate in the [School of Intelligent Science and Technology](http://hias.ucas.ac.cn/znkxyjs/index.htm), [Hangzhou Institute for Advanced Study (HIAS)](http://hias.ucas.ac.cn/), [University of Chinese Academy of Sciences (UCAS)](https://www.ucas.edu.cn/).

My research interests include long-lived, human-aware multimodal agents with a robust perception→cognition→feedback loop—via temporal cross-modal alignment, long-horizon memory & reasoning, multi-agent coordination, and safe embodied interaction.


<link href="https://fonts.googleapis.com/css2?family=Inter:wght@500;600&display=swap" rel="stylesheet">

<p class="phd-banner">
  I’m currently seeking PhD positions (Fall 2026 start) aligned with the focus above; I’m happy to connect regarding well-matched openings.
</p>

<style>
/* 仅影响这条声明，不改变全站样式 */
.phd-banner{
  font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  font-weight: 600;           /* 比正文略重，易扫读 */
  letter-spacing: 0.1px;      /* 轻微字距，提升可读性 */
  margin: 0.4rem 0 0.2rem;    /* 与前后段落留点呼吸感 */
}
</style>

I am very fortunate to be advised by [Prof. Taihao Li](https://people.ucas.ac.cn/~0070909) and [Dr. Leyuan Qu](https://people.ucas.edu.cn/~leyuanqu).

You can find my CV here: [Yu Liu's Curriculum Vitae](../assets/Yu_Liu_CV.pdf) (Updated: Oct 24, 2025).

<!-- [Email](mailto:liuyu233@mails.ucas.ac.cn) / [Github](https://github.com/YultheConkor) / [LinkedIn](https://www.linkedin.com/in/yu-liu-1b8004238/) -->

<section id="research-highlights">
  <h1>Research Highlights</h1>
  <p class="legend" aria-label="Legend">
  <em>Legend:</em>
  <span role="img" aria-label="Lead contribution">🌟</span> Lead contribution
  </p>
  <h2>Affective & Social AI</h2>
  <style>
  .legend { margin: 4px 0 14px; font-size: 0.92rem; color: #666; }
  .legend em { font-style: normal; font-weight: 600; margin-right: .5rem; color: #555; }
</style>
  <p><em>Research Highlights — Core: Multimodal Empathetic Dialogue Agent</em><br>
  <small>Role: Lead for multimodal fusion across text–audio–visual streams.</small><br>
  <small>Agent pipeline: Perception → Dialogue Understanding → Expression</small></p>

  <ul>
    <li>
      <strong>GRACE for Multimodal Facial Emotion Recognition</strong>
      [IEEE Transactions on Affective Computing <em>under review</em>, 
      <a href="https://arxiv.org/abs/2507.11892" target="_blank" rel="noopener">arXiv</a>] 🌟:
      <br>
      Introduces <em>GRACE</em> for dynamic facial emotion recognition by aligning refined linguistic cues with salient facial dynamics; achieves SOTA on DFEW, FERV39k, and MAFW.
      <em>Agent module: instant emotion perception for the dialogue agent.</em>
    </li>

    <li>
      <strong>Centering Emotion Hotspots</strong>
      [under review; <a href="https://arxiv.org/abs/2510.08606" target="_blank" rel="noopener">arXiv (Oct 2025)</a>] 🌟:
      <br>
      Extends GRACE from frame-level signals to conversation-level tracking by centering multimodal hotspots and fusing them with dialogue context, improving stability and generalization across turns.
      <em>Agent module: dialogue-level emotion understanding.</em>
    </li>

    <li>
      <strong>Think-Before-Draw</strong>
      [Pattern Recognition <em>under review</em> · 
      <a href="https://arxiv.org/abs/2507.12761" target="_blank" rel="noopener">arXiv</a>]:
      <br>
      A two-stage framework for disentangled, controllable talking-head synthesis. Designed the multimodal fusion module integrating audio–text–visual cues to preserve identity and sharpen affect control.
      <em>Agent module: controllable affective expression for responses.</em>
    </li>
  </ul>

  <h2>Digital Therapeutics</h2>

  
  <p><strong>Zhejiang Vanguard Project: Digital Therapeutics for Depression Detection</strong><br>
  <em>Research Highlight: Unified visual biomarker extraction with LLM-augmented questionnaire semantics via multimodal fusion for depression screening under privacy/low-resource constraints; <strong>incubated HOPE</strong> for subject-level estimation.</em> <br>
  <small>Role: Led multimodal fusion (video/audio/text); built the visual-biomarker pipeline; drove cross-module integration aligned to subject-level decisions.</small>
  </p>

<ul>
  <li>
    <strong>HOPE: Hierarchical Fusion for Optimized and Personality-Aware Estimation of Depression</strong>
    [<a href="https://doi.org/10.1145/3746027.3762063" target="_blank" rel="noopener">ACM MM ’25 · MPDD Challenge (Young Track) · DOI</a> ·
     <a href="https://github.com/YultheConkor/HOPE" target="_blank" rel="noopener">GitHub</a>] 🌟:
    <br>
    Subject-level depression detection under privacy constraints via hierarchical multimodal fusion (audio–video with personalized textual cues) and cross-task/sample consistency—<strong>1st place</strong> in the MPDD Young Track.<br>
    <em>Role:</em> Developed the consistency-aware subject-level fusion strategy; led the writing; completed final code integration and open-source release preparation.
  </li>
</ul>

  <h2>Earlier Work: Aviation Analytics</h2>
  <ul>
    <li>
      <strong>Risk Propagation on Flight Networks</strong>
      [<a href="https://doi.org/10.16097/j.cnki.1009-6744.2020.01.001" target="_blank" rel="noopener">Journal of Transportation Systems Engineering (2020) · DOI</a> 2023 Frontrunner 5000 Top Article]🌟:<br>
      Introduced Spearman correlation + SIR dynamics into airline operational risk management, yielding deployable complex-network propagation analysis.
    </li>
  </ul>
</section>

<section id="education">
  <h1>Education</h1>
  <ul>
    <li>
      University of Chinese Academy of Sciences — <strong>M.S. in Artificial Intelligence</strong>, Hangzhou Institute for Advanced Study (GPA 3.76/4.00),
      <em>Sep 2023 – Jun 2026 (expected)</em>.
    </li>
    <li>
      Civil Aviation University of China — <strong>B.S. in Transportation</strong> (GPA 3.36/4.00),
      <em>Sep 2015 – Jun 2019</em>.
    </li>
  </ul>
</section>

<section id="work-experience">
  <h1>Work Experience</h1>

  <h2>China Southern Airlines — Data Analyst, Airlines Operations Center <span style="font-weight:normal;">(Sep 2019 – May 2023)</span></h2>
  <ul>
    <li>
      Built Python/MySQL analytics and monitoring; automated Tableau/QuickBI reporting, reducing manual report time by <strong>&gt;70%</strong> and enabling shared data services across departments.
    </li>
    <li>
      Led a Django+SQL delay/fault analytics platform, cutting report generation time by <strong>~85%</strong> and supporting operational safety and decision-making.
    </li>
  </ul>

  <h2>Hangzhou Institute for Advanced Study, UCAS — Lab Coordinator &amp; Administrator <span style="font-weight:normal;">(Sep 2024 – Present)</span></h2>
  <ul>
    <li>
      Managed onboarding, event organization, and <strong>GPU resource</strong> scheduling; oversaw equipment procurement and reimbursement workflows to keep projects moving.
    </li>
  </ul>
</section>

<section id="professional-service">
  <h1>Professional Service</h1>
  <ul>
    <li><strong>Journal Reviewer</strong>: <em>Pattern Recognition</em> (Elsevier), <em>2025–present</em>.</li>
  </ul>
</section>